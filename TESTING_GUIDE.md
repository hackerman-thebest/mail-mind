# Mail-Mind Testing Guide

## The Problem: Resource-Constrained Test Machines

**Issue:** Running actual LLM inference requires significant resources:
- **RAM:** 4-8GB+ for model loading
- **CPU/GPU:** Significant compute for inference
- **Time:** 10-30 seconds per inference (first run even longer)
- **Disk:** 2-5GB for model files

**Result:** Test machines, CI/CD pipelines, and lightweight environments **cannot run real LLM inference** without:
- Timeouts
- Out of memory errors
- Slow test execution
- Infrastructure costs

## The Solution: Multi-Level Testing Strategy

We provide **4 levels of testing** from lightest to most realistic:

### Level 1: Unit Tests (No Ollama Required) âš¡ Fastest

**File:** `tests/unit/test_ollama_manager.py`

**Uses:** Object mocks via `unittest.mock`

**Pros:**
- âœ… Instant execution (<1 second)
- âœ… No dependencies
- âœ… Tests code logic
- âœ… Perfect for CI/CD

**Cons:**
- âŒ Doesn't test HTTP communication
- âŒ Doesn't validate API compatibility

**When to use:** Regular development, CI/CD, pre-commit hooks

**Run:**
```bash
pytest tests/unit/test_ollama_manager.py
```

---

### Level 2: Mock Client Tests (No Ollama Required) âš¡ Fast

**File:** `mock_ollama_test.py`

**Uses:** Enhanced object mocks with realistic behavior

**Pros:**
- âœ… Fast execution (2-3 seconds)
- âœ… No Ollama needed
- âœ… Realistic response generation
- âœ… Tests multiple scenarios

**Cons:**
- âŒ Still using mocks, not real HTTP

**When to use:** Local testing without Ollama, quick validation

**Run:**
```bash
python mock_ollama_test.py
```

---

### Level 3: Mock API Server (No Model Required) ðŸŽ¯ **RECOMMENDED**

**Files:**
- `mock_ollama_server.py` - HTTP server
- `test_with_mock_api.py` - Integration tests

**Uses:** Real HTTP server implementing Ollama API

**Pros:**
- âœ… Real HTTP communication
- âœ… Real `ollama` library usage
- âœ… No model download/inference needed
- âœ… Fast (5-10 seconds total)
- âœ… Tests actual integration code
- âœ… Validates API compatibility
- âœ… **Works on resource-constrained machines**

**Cons:**
- âŒ Not testing actual LLM behavior

**When to use:**
- âœ… Integration testing
- âœ… CI/CD pipelines
- âœ… Resource-constrained environments
- âœ… Development without GPU

**Run:**
```bash
# All-in-one (recommended)
python test_with_mock_api.py

# Or separately:
python mock_ollama_server.py --server-only  # Terminal 1
python main.py                               # Terminal 2
```

**What this tests:**
- âœ… OllamaManager initialization
- âœ… HTTP connection to API
- âœ… Model listing
- âœ… Connection pooling
- âœ… Inference requests (generate & chat)
- âœ… Streaming responses
- âœ… Priority classification logic
- âœ… Sentiment analysis logic
- âœ… Error handling

---

### Level 4: Real Ollama + Real Model (Full Stack) ðŸ¢ Slowest

**Requires:**
- Ollama installed
- Model downloaded (2-5GB)
- 4-8GB RAM available
- CPU/GPU for inference

**Pros:**
- âœ… Tests actual LLM behavior
- âœ… Real response quality
- âœ… True end-to-end validation

**Cons:**
- âŒ Slow (30+ seconds)
- âŒ High resource usage
- âŒ Not suitable for CI/CD
- âŒ Expensive infrastructure

**When to use:**
- Manual QA testing
- Production readiness validation
- Response quality assessment
- Performance benchmarking

**Setup:**
```bash
# 1. Install Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. Pull a lightweight model (recommended for testing)
ollama pull llama3.2:1b   # Smallest (1.3GB, fast)
# OR
ollama pull llama3.2:3b   # Balanced (2GB, good quality)

# 3. Run tests
python test_ollama_inference.py
```

---

## Recommended Testing Strategy

### For CI/CD Pipelines

```yaml
# .github/workflows/test.yml
- name: Unit Tests
  run: pytest tests/unit/

- name: Integration Tests (Mock API)
  run: python test_with_mock_api.py
```

**Why:** Fast, reliable, no resource constraints

### For Local Development

```bash
# Quick validation
python mock_ollama_test.py

# Integration testing
python test_with_mock_api.py

# (Optional) Full testing with real Ollama
python test_ollama_inference.py
```

### For Production Deployment

1. âœ… Run all mock tests (Levels 1-3)
2. âœ… Manual testing with real Ollama (Level 4)
3. âœ… User acceptance testing
4. âœ… Performance benchmarking

---

## Test Comparison Matrix

| Feature | Level 1 | Level 2 | Level 3 | Level 4 |
|---------|---------|---------|---------|---------|
| **Speed** | âš¡âš¡âš¡âš¡âš¡ | âš¡âš¡âš¡âš¡ | âš¡âš¡âš¡ | âš¡ |
| **Resources** | Minimal | Minimal | Low | **High** |
| **Realism** | 60% | 70% | **95%** | 100% |
| **CI/CD Suitable** | âœ… Yes | âœ… Yes | âœ… **Yes** | âŒ No |
| **HTTP Testing** | âŒ No | âŒ No | âœ… **Yes** | âœ… Yes |
| **Model Required** | âŒ No | âŒ No | âŒ **No** | âœ… Yes |
| **Tests Real LLM** | âŒ No | âŒ No | âŒ No | âœ… **Yes** |

---

## Troubleshooting

### "Test machine can't run LLM inference"

**Solution:** Use **Level 3 (Mock API Server)**
```bash
python test_with_mock_api.py
```

This gives you 95% realism without needing to run actual models.

### "CI/CD pipeline times out"

**Solution:** Use **Level 1 + Level 3**
```bash
pytest tests/unit/                # Fast unit tests
python test_with_mock_api.py      # Integration tests
```

### "Want to test response quality"

**Solution:** Use **Level 4** on your local machine
```bash
ollama pull llama3.2:3b
python test_ollama_inference.py
```

### "Out of memory errors"

**Options:**
1. Use lighter model: `ollama pull llama3.2:1b`
2. Use mock server (Level 3): `python test_with_mock_api.py`
3. Skip inference in tests: `export MAILMIND_SKIP_TEST=1`

---

## What Gets Tested at Each Level

### Code Logic âœ… (All Levels)
- Configuration handling
- Error handling
- Connection management
- Model fallback logic

### API Integration âœ… (Level 3+)
- HTTP requests/responses
- Streaming
- Error codes
- API compatibility

### Model Behavior âœ… (Level 4 Only)
- Actual LLM responses
- Response quality
- Inference performance
- Token generation

---

## Summary

**For Resource-Constrained Test Machines:**

Use **Level 3 (Mock API Server)** - it's the sweet spot:
- âœ… No model inference required
- âœ… Real HTTP communication tested
- âœ… Fast enough for CI/CD
- âœ… Validates integration correctness
- âœ… 95% realistic testing

**Command:**
```bash
python test_with_mock_api.py
```

**Result:** All Mail-Mind functionality tested without requiring 8GB RAM and GPU! ðŸŽ‰

---

## Test Scripts Reference

| Script | Purpose | Ollama Needed | Model Needed | Speed | Realism |
|--------|---------|---------------|--------------|-------|---------|
| `tests/unit/test_ollama_manager.py` | Unit tests | âŒ | âŒ | âš¡âš¡âš¡âš¡âš¡ | 60% |
| `mock_ollama_test.py` | Mock objects | âŒ | âŒ | âš¡âš¡âš¡âš¡ | 70% |
| `test_inference_scenarios.py` | Scenarios | âŒ | âŒ | âš¡âš¡âš¡âš¡ | 70% |
| `mock_ollama_server.py` | API server | âŒ | âŒ | âš¡âš¡âš¡ | 95% |
| `test_with_mock_api.py` | **Integration** | âŒ | âŒ | âš¡âš¡âš¡ | **95%** |
| `test_ollama_inference.py` | Full stack | âœ… | âœ… | âš¡ | 100% |
